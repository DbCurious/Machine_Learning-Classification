Random forests is a supervised learning algorithm. It can be used both for classification and regression. 
It is also the most flexible and easy to use algorithm. A forest is comprised of trees. 
It is said that the more trees it has, the more robust a forest is .Random forest consists of a large number of individual decision trees that operate as an ensemble. 
Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our modelâ€™s prediction.
A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.
So the prerequisites for random forest to perform well are:
   a.There needs to be some actual signal in our features so that models built using those features do better than random guessing.
   b.The predictions (and therefore the errors) made by the individual trees need to have low correlations with each other.
The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try 
   to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree.   

